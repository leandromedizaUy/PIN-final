name: 03 Create EKS Cluster

on:
  workflow_run:
    workflows: ["02 EC2 Config"]
    types:
      - completed

jobs:
  create_eks_cluster:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v1

      - name: Create Public Access Key
        run: echo "${{ secrets.PUBLIC_ACCESS_KEY }}" > remote-host-control.pub

      - name: Install Terraform
        env:
          TERRAFORM_VERSION: "1.10.5"
        run: |
          tf_version=$TERRAFORM_VERSION
          wget https://releases.hashicorp.com/terraform/"$tf_version"/terraform_"$tf_version"_linux_amd64.zip
          unzip terraform_"$tf_version"_linux_amd64.zip
          sudo mv terraform /usr/local/bin/

      - name: Terraform init
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: terraform init -input=false

      - name: Get EC2 DNS from Terraform output
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        id: get_dns
        run: |
          EC2_DNS=$(terraform output -raw remote_k8s_host_public_dns)
          echo "EC2_DNS=$EC2_DNS" >> $GITHUB_ENV
          echo "::set-output name=ec2_dns::$EC2_DNS"

      - name: Config SSH
        run: |
          echo "${{ secrets.EC2_SSH_KEY }}" > private_key.pem
          chmod 600 private_key.pem
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H $EC2_DNS >> ~/.ssh/known_hosts

      - name: Generate SSH Key and Create Cluster
        run: |
          ssh -i private_key.pem ${{ secrets.EC2_USER }}@$EC2_DNS << 'EOF'
            ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ""
            eksctl create cluster \
            --name eks-mundos-e \
            --without-nodegroup \
            --region us-east-1 \
            --zones us-east-1b,us-east-1a
          EOF
      - name: Create Node Group with SSH and IAM role
        run: |
          ssh -i private_key.pem ${{ secrets.EC2_USER }}@$EC2_DNS << 'EOF'
            eksctl create nodegroup \
            --cluster eks-mundos-e \
            --name workers-nodes \
            --node-type t3.medium \
            --nodes 3 \
            --nodes-min 1 \
            --nodes-max 3 \
            --ssh-access \
            --ssh-public-key ~/.ssh/id_rsa.pub \
            --managed=false 
          EOF
      - name: Attach EC2 permissions to EKS node role
        run: |
          ssh -i private_key.pem ${{ secrets.EC2_USER }}@$EC2_DNS << 'EOF'
            echo '{
            "Version": "2012-10-17",
            "Statement": [
                {
                  "Effect": "Allow",
                  "Action": [
                    "ec2:CreateVolume",
                    "ec2:DeleteVolume",
                    "ec2:AttachVolume",
                    "ec2:DetachVolume",
                    "ec2:DescribeVolumes",
                    "ec2:CreateTags"
                  ],
                  "Resource": "*"
                }
              ]
            }' > volume-policy.json
            ROLE_NAME=$(aws iam list-roles \
              --query 'Roles[?contains(RoleName, `eks-mundos-e-nodegroup-work`) == `true`].RoleName' \
              --output text)
  
            echo "Detected Role: $ROLE_NAME"
  
            aws iam put-role-policy \
              --role-name "$ROLE_NAME" \
              --policy-name EBSVolumeAccessPolicy \
              --policy-document file://volume-policy.json
          
            rm volume-policy.json
          EOF
